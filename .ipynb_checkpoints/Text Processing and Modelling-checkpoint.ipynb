{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlGiKXVB1N8D"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "# Worksheet 3\n",
    "\n",
    "**Motive:**\n",
    "\n",
    "This worksheet is meant to familiarize yourself with handling strings, regular expression, modeling loss function, linear regression. \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAwYcL-n1N8F"
   },
   "source": [
    "## Importing Packages\n",
    "\n",
    "__NOTE:__ For loading the packages into the current Python Jupyter notebook, use `import PACKAGE_NAME` command.  In case it throws an error i.e. `ModuleNotFoundError: No module named 'PACKAGE_NAME'`, then use `!pip install PACKAGE_NAME` in the code chunk to install the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snjN06lS1N8F"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%autosave 60\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (8, 5) #Note that we configure a custom default figure size.\n",
    "#Virtually every default aspect of matplotlib can be customized (https://matplotlib.org/users/customizing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOpwbrPG1N8G"
   },
   "source": [
    "## Importing Dataset\n",
    "**Fraudulent Emails  Dataset**: \n",
    "\n",
    "This dataset is a collection of more than 2,500 \"Nigerian\" Fraud Letters, dating from 1998 to 2007.\n",
    "\n",
    "These emails are in a single text file. Each e-mail has a header that includes the following information:\n",
    "\n",
    "- Return-Path: address the email was sent from\n",
    "- X-Sieve: the X-Sieve host (always cmu-sieve 2.0)\n",
    "- Message-Id: a unique identifier for each message\n",
    "- From: the message sender (sometimes blank)\n",
    "- Reply-To: the email address to which replies will be sent\n",
    "- To: the email address to which the e-mail was originally set (some are truncated for anonymity)\n",
    "- Date: Date e-mail was sent\n",
    "- Subject: Subject line of e-mail\n",
    "- X-Mailer: The platform the e-mail was sent from\n",
    "- MIME-Version: The Multipurpose Internet Mail Extension version\n",
    "- Content-Type: type of content & character encoding\n",
    "- Content-Transfer-Encoding: encoding in bits\n",
    "- X-MIME-Autoconverted: the type of autoconversion done\n",
    "- Status: r (read) and o (opened)\n",
    "\n",
    "Use the link (\"https://raw.githubusercontent.com/sukhjitsehra/datasets/master/DATA200/Datasets/Fraudulent_Emails.txt\") to scrap the dataset into Python session using `requests` library and name the dataset as \"email_text\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "g0imuvPO1N8G",
    "outputId": "c3278c7d-1982-4ee2-806a-755982c21013",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "URL=\"https://raw.githubusercontent.com/sukhjitsehra/datasets/master/DATA200/Datasets/Fraudulent_Emails.txt\"\n",
    "response = requests.get(URL)\n",
    "email_text= response.text #It may take a while to download the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAKKADJw1N8I",
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Strings Manipulation in Python\n",
    "\n",
    "Most of the strings operations can be easily made by using built-in functions provided by Python, you can visit [here](https://bohr.wlu.ca/cp104/notes/strings.php) for the basic methods for string manipulation. For more complex cases of matching and manipulation, it is necessary to use regular expressions. Regular expressions provide a very flexible way to search and match string patterns within text. A single expression, generically called `regex`, is a string formed according to the regular expression language. We can use the following metacharacters that have a special meaning:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Char   | Description                         | Example                    | Matches        | Doesn't Match |\n",
    "| ------ | ----------------------------------- | -------------------------- | -------------- | ------------- |\n",
    "| .      | Any character except \\n             | `...`                      | abc            | ab<br>abcd    |\n",
    "| [ ]    | Any character inside brackets       | `[cb.]ar`                  | car<br>.ar     | jar           |\n",
    "| [^ ]   | Any character _not_ inside brackets | `[^b]ar`                   | car<br>par     | bar<br>ar     |\n",
    "| \\*     | ≥ 0 or more of last symbol          | `[pb]*ark`                 | bbark<br>ark   | dark          |\n",
    "| +      | ≥ 1 or more of last symbol          | `[pb]+ark`                 | bbpark<br>bark | dark<br>ark   |\n",
    "| ?      | 0 or 1 of last symbol               | `s?he`                     | she<br>he      | the           |\n",
    "| {_n_}  | Exactly _n_ of last symbol          | `hello{3}`                 | hellooo        | hello         |\n",
    "| &#124; | Pattern before or after bar         | <code>we&#124;[ui]s</code> | we<br>us<br>is | e<br>s        |\n",
    "| \\      | Escapes next character              | `\\[hi\\]`                   | [hi]           | hi            |\n",
    "| ^      | Beginning of line                   | `^ark`                     | ark two        | dark          |\n",
    "| \\$     | End of line                         | `ark$`                     | noahs ark      | noahs arks    | \n",
    "\n",
    "\n",
    "Further, to extend the above table, a set of characters inside a pair of square brackets [...] have a special meaning, called character class:\n",
    "\n",
    "Set|Description\n",
    "---|----------------\n",
    "[arn]|\tReturns a match where one of the specified characters (a, r, or n) are present\t\n",
    "[a-n]|\tReturns a match for any lower case character, alphabetically between a and n\t\n",
    "[^arn]|\tReturns a match for any character EXCEPT a, r, and n\t\n",
    "[0123]|\tReturns a match where any of the specified digits (0, 1, 2, or 3) are present\t\n",
    "[0-9]|\tReturns a match for any digit between 0 and 9\t\n",
    "[0-5][0-9]|\tReturns a match for any two-digit numbers from 00 and 59\t\n",
    "[a-zA-Z]|\tReturns a match for any character alphabetically between a and z, lower case OR upper case\t\n",
    "[+]|\tIn sets, +, *, ., \\|, (), $,{} has no special meaning, so [+] means: return a match for any + character in the string\t\n",
    "\n",
    "\n",
    "Regular expressions use quantifiers that allow us to match multiple consecutive appearances of a pattern. We specify the number of repetitions by placing the number in curly braces `{ }`.\n",
    "\n",
    "Quantifier | Meaning\n",
    "--- | ---\n",
    "{m, n} | Match the preceding character m to n times.\n",
    "{m} | Match the preceding character exactly m times.\n",
    "{m,} | Match the preceding character at least m times.\n",
    "{,n} | Match the preceding character at most n times.\n",
    "\n",
    " A special sequence is a \\ followed by one of the characters in the list below has a special meaning in regular expressions:\n",
    "\n",
    "Character|Description|Example\n",
    "---------|-----------|-------\n",
    "\\A|\tReturns a match if the specified characters are at the beginning of the string|\t\"\\AThe\"\t\n",
    "\\b|\tReturns a match where the specified characters are at the beginning or at the end of a word| r\"\\bain\" r\"ain\\b\"\t\n",
    "\\B|\tReturns a match where the specified characters are present, but NOT at the beginning (or at the end) of a word |\tr\"\\Bain\" r\"ain\\B\"\t\n",
    "\\d|\tReturns a match where the string contains digits (numbers from 0-9)\t|\"\\d\"\t\n",
    "\\D|\tReturns a match where the string DOES NOT contain digits\t|\"\\D\"\t\n",
    "\\s|\tReturns a match where the string contains a white space character\t|\"\\s\"\t\n",
    "\\S|\tReturns a match where the string DOES NOT contain a white space character\t|\"\\S\"\t\n",
    "\\w|\tReturns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\t|\"\\w\"\t\n",
    "\\W|\tReturns a match where the string DOES NOT contain any word characters\t|\"\\W\"\t\n",
    "\\Z|\tReturns a match if the specified characters are at the end of the string\t|\"Spain\\Z\"\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAKKADJw1N8I",
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "There is a built-in Python module called `re`, which is responsible for the operation of the regex.\n",
    "\n",
    "The `re` module provides a set of functions that can be divided into three categories: \n",
    "- Pattern matching\n",
    "- Substitution\n",
    "- Splitting\n",
    "\n",
    "## RegEx Functions\n",
    "The `re` module offers a set of functions that allows us to search a string for a match:\n",
    "\n",
    "Function | Description\n",
    "---------| -----------\n",
    "findall| Returns a list containing all matches, if no matches are found, an empty list is returned\n",
    "search|\tReturns a Match object if there is a match anywhere in the string\n",
    "split|\tReturns a list where the string has been split at each match\n",
    "sub|\tReplaces one or many matches with a string\n",
    "\n",
    "\n",
    "The `search(...)` function returns a match object containing information about the search and its result.\n",
    "\n",
    "*Note*: If there is no match, the value None will be returned, instead of the Match Object.\n",
    "\n",
    "The Match object has properties and methods used to retrieve information about the search, and the result:\n",
    "\n",
    "Method|Description\n",
    "------|-----------\n",
    ".span()| returns a tuple containing the start, and end positions of the match.\n",
    ".string| returns the string passed into the function\n",
    ".group()| returns the part of the string where there was a match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Question 1: \n",
    "\n",
    "The scrapped data `email_text` has following format:\n",
    "\n",
    "        From r  Wed Oct 30 21:41:56 2002\n",
    "        Return-Path: <james_ngola2002@maktoob.com>\n",
    "        X-Sieve: cmu-sieve 2.0\n",
    "        Return-Path: <james_ngola2002@maktoob.com>\n",
    "        Message-Id: <200210310241.g9V2fNm6028281@cs.CU>\n",
    "        From: \"MR. JAMES NGOLA.\" <james_ngola2002@maktoob.com>\n",
    "        Reply-To: james_ngola2002@maktoob.com\n",
    "        To: webmaster@aclweb.org\n",
    "        Date: Thu, 31 Oct 2002 02:38:20 +0000\n",
    "        Subject: URGENT BUSINESS ASSISTANCE AND PARTNERSHIP\n",
    "        X-Mailer: Microsoft Outlook Express 5.00.2919.6900 DM\n",
    "        MIME-Version: 1.0\n",
    "        Content-Type: text/plain; charset=\"us-ascii\"\n",
    "        Content-Transfer-Encoding: 8bit\n",
    "        X-MIME-Autoconverted: from quoted-printable to 8bit by sideshowmel.si.UM id g9V2foW24311\n",
    "        Status: O\n",
    "\n",
    "        FROM:MR. JAMES NGOLA.\n",
    "        CONFIDENTIAL TEL: 233-27-587908.\n",
    "        E-MAIL: (james_ngola2002@maktoob.com).\n",
    "\n",
    "        Body of Email.\n",
    "\n",
    "Write Python code to:\n",
    " \n",
    "a) Search if \"Portugal\" is ever discussed in the `email_text` data. If Yes, print that `Portugal is found in the data`.  _Hint: Use re.search(PATTERN, STRING)._\n",
    "\n",
    "b) Split the `email_text` string at `From r` location. _HINT: Use re.split(PATTERN, STRING)_\n",
    "\n",
    "c) Extract the `From: addresses` part from the `email_text` data. _Hint: Use re.findall(PATTERN, STRING) and for loops to print the results._\n",
    "\n",
    "d)  Extract the email addresses from the `email_text` data.\n",
    "\n",
    "<div>\n",
    "    \n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a)\n",
    "match = re.search(\"Portugal\", email_text) # SOLUTION\n",
    "if match is not None:   # SEED\n",
    "    print(\"Portugal is found in the data\")  # SEED\n",
    "else:  # SEED\n",
    "    print(\"Portugal is not found in the data\")  # SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b)\n",
    "split=re.split('From r', email_text) # SOLUTION\n",
    "split[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c)\n",
    "for line in re.findall('From:.*', email_text):  # SEED\n",
    "    print(line)  # SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d)\n",
    "re.findall(\"\\w\\S*@.*\\w\", email_text) # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and Regex in Pandas\n",
    "\n",
    "When we load the data using Pandas, one important thing to note here is that object datatype is still the default datatype for strings. To use it as StringDtype, we need to explicitly state it e.g. converting to “string” using astype function.\n",
    "\n",
    "Pandas provide a set of string functions that make it easy to operate on string data. Most importantly, these functions ignore (or exclude) missing/NaN values. These functions/methods can be used via `DATAFRAME.str.METHOD_NAME()` call.\n",
    "\n",
    "Almost, all of these methods work with Python string functions. So, convert the Series Object to String Object and then perform the operation.\n",
    "\n",
    "- lower(): Converts strings in the Series/Index to lower case.\t\n",
    "- upper(): Converts strings in the Series/Index to upper case.\n",
    "- len(): Computes String length().\n",
    "- strip(): Helps strip whitespace(including newline) from each string in the Series/index from both sides.\n",
    "- cat(sep=' '): Concatenates the series/index elements with given separator.\n",
    "- get_dummies(): Returns the dataframe with One-Hot Encoded values.\n",
    "- contains(pattern): Returns a Boolean value True for each element if  pattern or regex is contained within a string of a Series or Index else False. \n",
    "- repeat(value): Repeats each element with a specified number of times.\n",
    "- swapcase: Swaps the case lower/upper.\n",
    "- islower(): Checks whether all characters in each string in the Series/Index in lower case or not. Returns Boolean\n",
    "- isupper(): Checks whether all characters in each string in the Series/Index in upper case or not. Returns Boolean.\n",
    "- isnumeric(): Checks whether all characters in each string in the Series/Index are numeric. Returns Boolean.\n",
    "\n",
    "The following methods provide mapping between Pandas methods and functions in Python’s `re` module:\n",
    "\n",
    "- find(pattern): Returns the first position of the first occurrence of the pattern.\n",
    "- findall(pattern): Returns a list of all occurrences of pattern or regular expression in the Series/Index. Equivalent to applying re.findall() on all elements\n",
    "- match(pattern): Returns matched groups as a list. Calls re.match() and returns a boolean.\n",
    "- extract(pattern): Returns DataFrame with one row for each element and one column for each regex capture group.\n",
    "- extractall(pattern): Returns DataFrame with one row for each match and one column for each regex capture group.\n",
    "- split(pattern): Splits each string with the given pattern. Equivalent to str.split().\n",
    "- rsplit(pattern): Equivalent to str.rsplit(), but accepts regexps\n",
    "- replace(a,b): Replace the search string or pattern  a with the value b.   \n",
    "- count(pattern): Returns the count occurrences of pattern in each string of the Series/Index\n",
    "- startswith(pattern): Returns true if the element in the Series/Index starts with the pattern.\n",
    "- endswith(pattern): Returns true if the element in the Series/Index ends with the pattern.\n",
    "\n",
    "For working with above functions, lets load the dataframe version of `email_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://raw.githubusercontent.com/sukhjitsehra/datasets/master/DATA200/Datasets/Fraudulent_Emails_DF.csv\"\n",
    "email_df=pd.read_csv(URL)\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Question 2: \n",
    "\n",
    "Write Python code to:\n",
    "\n",
    "a) Remove the rows from `email_df` with NaN values.\n",
    "\n",
    "b) Please report the observation(s) where the sender's email contains `brown` or `bala` or `ahmedakim`. \n",
    "\n",
    "_Hint: Use .str.contains(\"TEXT_1|TEXT_2|TEXT_3\") method to filter the observations those contains the required text._\n",
    "\n",
    "c) In `recipient_name` column replace the entries with text `Rrrrr` and `rrrrr` with `UNKNOWN` text.  \n",
    "\n",
    "_Hint: Use .str.replace(r'REGEX_1 | REGEX_2', REPLACEMENT_TEXT, regex=True, inplace = True) method for replacement._\n",
    "\n",
    "d) Report the observation(s) where the recipient email starts from with a digit. \n",
    "\n",
    "_Hint: Use .str.match(REGEX) method_.\n",
    "\n",
    "e) Find the length of the `sender_email` in `email_df` dataset and create a histogram to look at the distribution of the length of emails used. \n",
    "\n",
    "_Hint: Use str.len().plot(kind=\"hist\") method_.\n",
    "\n",
    "f) Create a new column \"year\" to `email_df` by extracting it from `date_sent` column of `email_df`.\n",
    "\n",
    "\n",
    "<div>\n",
    "    \n",
    "``` \n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a)\n",
    "email_df=email_df.dropna() # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b)\n",
    "email_df[email_df[\"sender_email\"].str.contains(\"brown|bala|ahmedakim\")] # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c)\n",
    "email_df[\"recipient_name\"].replace(r'r{4,5}|Rr{4}','UNKNOWN', regex=True, inplace = True) # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d)\n",
    "email_df[email_df[\"recipient_email\"].str.match('^\\d+')] # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part e) \n",
    "email_df.sender_email.str.len().plot(kind=\"hist\") # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part f)\n",
    "email_df[\"year\"] = (email_df.date_sent\n",
    ".str.strip()\n",
    ".str.extract('(\\d{4})')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Modeling, Summary Statistics and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Constant Model and Loss Functions\n",
    "\n",
    "### Constant Model\n",
    "\n",
    "In the modeling context, $y$ represents our \"true observations\", which we are trying to model. $\\hat{y}$ represents our prediction (for any model). In this worksheet, we will use the constant model, where our prediction for any input is a constant:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta\n",
    "$$\n",
    "\n",
    "$\\theta$ is what we call a **parameter**. Our goal is to find the value of our parameters that **best fit our data**. We represent the optimal parameter(s) with $\\hat{\\theta}$.\n",
    "\n",
    "We call the constant model a **summary statistic**, as we are determining one number that best \"summarizes\" a set of values.\n",
    "\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Loss functions are what we use to determine the optimal parameter(s) for our model.\n",
    "\n",
    "A loss function is a measure of how well a model can predict the expected outcome. In other words, it measures the deviations of the predicted values from the observed values. We will implement the squared loss and absolute loss functions.  \n",
    "\n",
    "In the formulations below $y$ represents the observed values and $\\hat{y}$ stands for our prediction.\n",
    "\n",
    "1. **Squared Loss** (also known as the $L_2$ loss, pronounced \"ell-two\"):\n",
    "\n",
    "$$\\Large L(y, \\hat{y}) = (y - \\hat{y})^2$$\n",
    "\n",
    "2. **Absolute Loss** (also known as the $L_1$ loss, pronounced \"ell-one\"):\n",
    "\n",
    "$$\\Large L\\left(y, \\hat{y} \\right) = \\left| y - \\hat{y} \\right|$$\n",
    "\n",
    "Since we are using the constant model $\\hat{y} = \\theta$, we will instead refer to these loss functions as being $(y - \\theta)^2$ and $|y - \\theta|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "### Question 3: Implement the squared loss function\n",
    "\n",
    "\n",
    "$$\\Large\n",
    "L\\left(y,  \\theta \\right) = \\left( y - \\theta \\right)^2\n",
    "$$\n",
    "\n",
    "Based on the comments below, implement the squared loss function. Your answer should not use any loops.\n",
    "    \n",
    "<div>    \n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_obs, theta):\n",
    "    \"\"\"\n",
    "    Calculate the squared loss of the observed data and a summary statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    y_obs: an observed value\n",
    "    theta : some constant representing a summary statistic\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The squared loss between the observation and the summary statistic.\n",
    "    \"\"\"\n",
    "    return (y_obs - theta)**2 # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Mean Squared Error for the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Let's apply our knowledge to some real-world data. Below you are given an array of Median values of owner-occupied homes in \\\\$1000's from a boston dataset. This dataframe contains the following columns:\n",
    "- `crim` per capita crime rate by town.\n",
    "- `zn` proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- `indus` proportion of non-retail business acres per town.\n",
    "- `chas` Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "- `nox` nitrogen oxides concentration (parts per 10 million).\n",
    "- `rm` average number of rooms per dwelling.\n",
    "- `age` proportion of owner-occupied units built prior to 1940.\n",
    "- `dis` weighted mean of distances to five Boston employment centres.\n",
    "- `rad` index of accessibility to radial highways.\n",
    "- `tax` full-value property-tax rate per \\$10,000.\n",
    "- `ptratio` pupil-teacher ratio by town.\n",
    "- `black` $1000(Bk-0.63)^2$ where Bk is the proportion of certain population by town.\n",
    "- `lstat` lower status of the population (percent).\n",
    "- `medv` median value of owner-occupied homes in \\$1000s.\n",
    "\n",
    "In this section, you will try to find the best statistic $\\theta$ to represent the median value of owner-occupied homes given in the array. The procedure includes constructing the mean squared error (MSE) for the `medv` data and finding the value that minimizes the MSE. \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://raw.githubusercontent.com/sukhjitsehra/datasets/master/DATA200/Datasets/Boston.csv\"\n",
    "boston=pd.read_csv(URL)\n",
    "medv=np.array(boston['medv']) # array of observed median value of owner-occupied homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend the above loss functions to an entire dataset by taking the average. Let the dataset $\\mathcal{D}$ be the set of observations:\n",
    "\n",
    "$$\\Large\\mathcal{D} = \\{y_1, \\ldots, y_n\\}$$\n",
    "\n",
    "where $y_i$ is the $i^{th}$ median value of the owner-occupied home.\n",
    "\n",
    "We can define the average loss over the dataset as:\n",
    "\n",
    "$$\\Large\n",
    "R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Define the `mean_squared_error` function which computes the mean squared error given the data and a value for `theta`. Assume that `data` will be a numpy array.\n",
    "\n",
    "_Hint:_ For each observation call squared_loss(...) function. You may use list comprehension to shorten the code. \n",
    "\n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(theta, data):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error of a value for theta and the observed data.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta : some constant representing a summary statistic\n",
    "    data: numpy array of the observed data\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean squared error of a value for theta and the observed data.\n",
    "    \"\"\"\n",
    "    return np.mean([squared_loss(theta, d) for d in data]) # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "In the cell below, plot the mean squared error for different `theta` values. Note that `theta_values` are given. Make sure to label the axes (label $\\theta$ to x-axis and `L2 loss` to y-axis) on your plot. Remember to use the `medv` variable we defined earlier\n",
    "    \n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_values = np.linspace(0, 45, 100)\n",
    "mse = ...\n",
    "# BEGIN SOLUTION\n",
    "mse = [mean_squared_error(theta, medv) for theta in theta_values]\n",
    "plt.plot(theta_values, mse)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L2 loss')\n",
    "plt.title(r'L2 Loss for different values of $\\theta$');\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Based on the above results, find the value of `theta` that minimizes the L2 loss above via observation of the plot you've generated. Round your answer to two decimal places.\n",
    "\n",
    "_Hint: Use THETAS[np.argmin(L2_LOSS)] by replacing capital letters appropriately._\n",
    "    \n",
    "<div>\n",
    "    \n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4c\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_observed_mse = round(theta_values[np.argmin(mse)],2)  # SOLUTION\n",
    "min_observed_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "We know that the value of `theta` that minimizes the mean squared error is the average of the data for the constant model. Assign `min_computed` to the mean of the `medv` dataset, and compare this to the values you observed in question 4c.\n",
    "\n",
    "<div>\n",
    "    \n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4d\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_computed = np.mean(medv) # SOLUTION\n",
    "min_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've hopefully convinced yourself that the `mean` of the data is the summary statistic that minimizes mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5: Implement the Absolute Loss \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Please follow the exact same steps as above but for the absolute loss function. Absolute loss is defined as:\n",
    "\n",
    "$$\\Large\n",
    "L\\left(y, \\theta \\right) = \\left| y - \\theta \\right|\n",
    "$$\n",
    "\n",
    "<div>\n",
    "    \n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "manual: true\n",
    "```\n",
    "In the cell below define the function `abs_loss` which returns the absolute loss given a value of `theta` and `y_obs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(theta, y_obs):\n",
    "    \"\"\"\n",
    "    Calculate the absolute loss of the observed data and a summary statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    y_obs: an observed value\n",
    "    theta : some constant representing a summary statistic\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The absolute loss between the observation and the summary statistic.\n",
    "    \"\"\"\n",
    "    return np.abs(theta - y_obs) # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Question**: How are outliers penalized differently in absolute loss compared to square loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a: Mean Absolute Error for the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Define the `mean_absolute_error` function which computes the mean absolute error given the data and a value for `theta`. Assume that `data` will be a numpy array.\n",
    "    \n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(theta, data):\n",
    "    \"\"\"\n",
    "    Calculate the mean absolute error of a value for theta and the observed data.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta : some constant representing a summary statistic\n",
    "    data: numpy array of the observed data\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean absolute error of a value for theta and the observed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean([abs_loss(theta, d) for d in data]) # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6b\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "In the cell below, plot the mean absolute error for different `theta` values on the `medv` dataset. Note that `theta_values` are given. Make sure to label the axes on your plot (label $\\theta$ to x-axis and `L1 loss` to y-axis) on your plot.\n",
    "\n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_values = np.linspace(0, 45, 100)\n",
    "mae = ...\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "mae = [mean_absolute_error(theta, medv) for theta in theta_values]\n",
    "plt.plot(theta_values, mae)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('L1 loss')\n",
    "plt.title(r'L1 Loss for different values of $\\theta$');\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the plot looks somewhat similar the plot of the mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6c\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Find the `theta` value that minimizes L1 loss. Round up to 2 decimal places.\n",
    "\n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q6c\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_observed_mae = round(theta_values[np.argmin(mae)],2) # SOLUTION\n",
    "min_observed_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6d: Find the Minimizing Value Using Absolute Error\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "We know that the value of `theta` that minimizes the mean absolute error is the median value of the data for the constant model. Assign `min_abs_computed` to the median of the `medv` dataset, and compare this to the values you observed in questions 6d.\n",
    "    \n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q6d\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_abs_computed = np.median(medv) # SOLUTION\n",
    "min_abs_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've hopefully convinced yourself that the `median` of the data is the summary statistic that minimizes mean absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Steps for developing a statistical model\n",
    "\n",
    "1. Data Collection\n",
    "\n",
    "    - The quantity & quality of your data dictates how accurate the model is. One can have primary data or can use secondary data for model training and testing. \n",
    "    \n",
    "1. Data Preparation\n",
    "    -  Creating technical data from raw data and then consistent data for statistical analysis. \n",
    "    - Cleaning may be required (remove duplicates, correct errors, deal with missing values, normalization, data type conversions, etc.).\n",
    "    - Visual inspection of the data and identify the relationship between the features. Checking for class imbalance and other exploratory analysis. \n",
    "    - Choosing the correct methodology of training and testing the models. Splitting the data into Train and Test data sets.  \n",
    "    \n",
    "1.  Choose a Model\n",
    "     - Choose the model classes based on the dependent variable type. For instance, if the dependent variable is quantitative then regression models can be selected. Different algorithms are used for different tasks; choose the right one.\n",
    " \n",
    "1.  Train the Model\n",
    "\n",
    "    - The goal of training is to answer a question or make a prediction correctly as often as possible.\n",
    " \n",
    "1.  Evaluate the Model\n",
    "\n",
    "    - Based on the metrics or combination of metrics, objective performance of the model is measured. \n",
    "    - Test the model against previously unseen data.\n",
    "    - This unseen data is meant to be somewhat representative of model performance in the real world, but still helps to tune the model (as opposed to testing data, which does not).\n",
    "    \n",
    "1. Parameter Tuning\n",
    "\n",
    "    - Some models do have hyper-parameters, those need to be tuned for improving the performance. \n",
    "\n",
    "1. Make Predictions\n",
    "\n",
    "    - Using test data, make predictions to see the model performance. \n",
    "\n",
    "__Note: It may be needed to go back from the last step to the first step, if the model is not performing the way it was expected.__\n",
    "\n",
    "### Exploratory Data Analysis \n",
    "\n",
    "Let's move to regression model building, but before that, it would be a good idea to look at data and do some exploratory data analysis of the dataset. For regression modeling, we would be using `boston` dataset loaded in the code chunks above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "a) How many rows and columns are in the `boston` data set? What do the rows and columns represent?\n",
    "\n",
    "b) Create pairwise scatterplots of the predictors (columns) in this data set. Are any of the predictors associated with median value of owner-occupied homes? If so, explain the relationship.\n",
    "\n",
    "c) Do any of the suburbs of Boston appear to have particularly high median value of owner-occupied homes? Tax rates? Pupil-teacher ratios? \n",
    "\n",
    "d) How many of the suburbs in this data set bound the Charles river? and What is the median pupil-teacher ratio among the towns in this data set?\n",
    "\n",
    "e) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.\n",
    "\n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a)\n",
    "boston.shape # SOLUTION\n",
    "boston.columns # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b)\n",
    "# BEGIN SOLUTION\n",
    "sns.pairplot(boston) \n",
    "#`lstat` and `rm` seem strongly associated with `medv`\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c)\n",
    "\n",
    "# add 1x3 matplotlib grid and plot 3 seaborn distplot\n",
    "f, axes = plt.subplots(1, 3)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "sns.distplot(boston['medv'],  ax=axes[0])\n",
    "sns.distplot(boston['tax'], ax=axes[1])\n",
    "sns.distplot(boston['ptratio'], ax=axes[2]);\n",
    "print(boston[['medv', 'tax', 'ptratio']].describe())\n",
    "# Based on the above results, lets look at the high values\n",
    "print(\"Number of high median value of owner-occupied homes: {}\".format(boston[boston['medv'] > 40][\"medv\"].count()))\n",
    "print(\"Number of high tax rates: {}\".format(boston[boston['tax'] > 700][\"tax\"].count()))\n",
    "print(\"Number of high Pupil-teacher ratios: {}\".format(boston[boston['ptratio'] > 21][\"ptratio\"].count()))\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d)\n",
    "\n",
    "boston['chas'].value_counts() # SOLUTION\n",
    "\n",
    "boston['ptratio'].median()  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part e)\n",
    "\n",
    "boston[boston['rm'] > 7] # SOLUTION\n",
    "\n",
    "boston[boston['rm'] > 8] # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "We fit above a **constant** model to this dataset, meaning our model was $\\hat{y} = \\theta$. In other words, given the boston dataset, we tried to find a summary statistic $\\theta$ that best represented our set of the median value of owner-occupied homes. To find the value of $\\theta$, we minimized the following empirical risk:\n",
    "\n",
    "$$R(\\theta) = \\frac{1}{n}\\sum_{i = 1}^n L(y_i, \\theta)$$\n",
    "\n",
    "Here, $\\mathcal{D} = \\{y_1, y_2, ..., y_n \\}$ refers to our set of `medv` values.\n",
    "\n",
    "We looked at two different loss functions:\n",
    "\n",
    "- $L_2$: $L_2(y_i, \\hat{y_i}) = (y_i - \\hat{y_i})^2$\n",
    "\n",
    "- $L_1$: $L_1(y_i, \\hat{y_i}) = \\left| y_i - \\hat{y_i} \\right|$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We have seen above in question 7 that `lstat` and `rm` seem strongly associated with `medv` from boston dataset.  Specifically, we're interested in the relationship between the ` lower status of the population (percent)` (lstat) column and `medv` column. Our goal will be to predict medv ($y$) from lstat ($x$), i.e., we want to find values of $a$ and $b$ so that given $x$, predict $y$ as\n",
    "$$\\boxed{\\hat{y} = a + bx}$$\n",
    "We will now explore different ways to obtain the optimal values of $a, b$, called $\\hat{a}, \\hat{b}$, where $\\hat{y} = \\hat{a} + \\hat{b}x$. \n",
    "\n",
    "In real world data science work, you are far more likely to use something similar to the `seaborn` and `scikit-learn` approaches.\n",
    "\n",
    "First, let's run `sns.lmplot`, which will both provide a scatterplot of `medv` vs `lstat` and also display the least-squares line of best fit. This line of best fit that we would look to determine empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = boston, x = \"lstat\", y = \"medv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to use the package scikit-learn. It is a widely used Python library for machine learning, built on top of NumPy and some other packages. It provides the means for preprocessing data, reducing dimensionality, implementing regression, classification, clustering, and more. Like NumPy, scikit-learn is also open source.\n",
    "\n",
    "To fit the linear regression mode, we first create a `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `model` is the instance of LinearRegression. You can provide several optional parameters to LinearRegression:\n",
    "\n",
    "- `fit_intercept` is a Boolean (True by default) that decides whether to calculate the intercept (True) or consider it equal to zero (False).\n",
    "- `normalize` is a Boolean (False by default) that decides whether to normalize the input variables (True) or not (False).\n",
    "- `copy_X` is a Boolean (True by default) that decides whether to copy (True) or overwrite the input variables (False).\n",
    "- `n_jobs` is an integer or None (default) and represents the number of jobs used in parallel computation. None usually means one job and -1 to use all processors.\n",
    "\n",
    "For simple regression model fitting, we are using the default values of all parameters. Therefore, the `model` is like a \"blank slate\" for a linear model. \n",
    "\n",
    "Now, we need to tell `model` to \"fit\" itself to the data. The .fit() method, calculates the optimal values of the weights $\\hat a$ and $\\hat b$ values, using the existing input and output (X and y) as the arguments. In other words, .fit() fits the model. It returns self, which is the variable model itself. \n",
    "\n",
    "<i>Note: `X` needs to be a matrix (or DataFrame), as opposed to a single array (or Series). This is because `sklearn.linear_model` is robust enough to be used for multiple regression.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X = boston[['lstat']], y= boston['medv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model exists, we can look at the $\\hat a$ and $\\hat b$ values it found, by attributes`intercept_` and `coef_`, respectively. Here,\n",
    "\n",
    " - coef_: It is used to return the coefficients for the linear regression problem.\n",
    " - Intercept_: Intercept is an independent term in this linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your model fitted, you can get the results to check whether the model works satisfactorily and interpret it.\n",
    "\n",
    "You can obtain the coefficient of determination (𝑅²) with `.score(X,y)` called on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsq = model.score(X = boston[['lstat']], y= boston['medv'])\n",
    "\n",
    "Rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once there is a satisfactory model, you can use it for predictions with either existing or new data.\n",
    "\n",
    "To use the `scikit-learn` linear regression model to make predictions, you can use the `model.predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X = boston[['lstat']]) # X needs to be a 2D array since the X above was also a 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Create a scatterplot for `lstat` vs `medv` from boston dataset and also overlay a line for `lstat` vs `predictions` calculated above. Does this graph looks similar to  lmplot(...) in the examples above.\n",
    "\n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q8\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "sns.scatterplot(x='lstat', y='medv', data=boston)\n",
    "plt.plot(boston[\"lstat\"],  predictions, color = 'r');\n",
    "# Yes, this matches with the one created using lmplot(...) above.\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "In the previous sections, we learnt how to establish relationships between one independent explanatory variable and one response variable. However, with real-world problems, you will often want to use **multiple features** to model and predict a response variable. To do so, we will use multiple linear regression, which attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data. Formally, the model for multiple linear regression, given $p$ features is:\n",
    "\n",
    "$$y_i = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + … + \\theta_p x_p $$\n",
    "\n",
    "Please note that we have been using the terms **features**, **independent variables**, and **explanatory variables** interchangeably. Usually “features” are used in the context of machine learning when you are trying to make predictions. “Independent variables” and “explanatory variables” are mainly found in statistics, econometrics and other related fields that focus on understanding the relationship between a set of variables.  \n",
    "\n",
    "### Creating train and test data for validation \n",
    "\n",
    "Before we fit the regression model, let's split the boston dataset into `train` dataset (used to fit the model) and `test` dataset (to evaluate the model). \n",
    "\n",
    "There are several ways to proportionally split our data into train and test sets: 50/50, 60/40, 70/30, 80/20, and so forth. The data split that you select should be based on your experience and judgment. For this worksheet, I will use a 70/30 split. This approach is called the validation set approach (or data split). \n",
    "\n",
    "For this we are going to use `train_test_split` method from sklearn.model_selection, which separates predictors, response variables for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.drop(['medv'], axis=1), boston['medv'], test_size = 0.3, random_state = 101)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "a) Using scikit learn's `LinearRegression`, create and fit a model (name it `model_multiple`) that tries to predict `medv` from `lstat` AND `rm` from `train` dataset. \n",
    "\n",
    "b) Report the coefficients and intercept of the `model_multiple` regression model. \n",
    "\n",
    "c) Use the results from part b) to write out the function model equation to predict medv from lstat and rm.\n",
    "\n",
    "d) Predict the median value of owner-occupied homes for new unseen data (i.e. `X_test` data). \n",
    "\n",
    "e) Report the $R^2$ using for the `model_multiple` regression model and compare it with $R^2$ for `model` regression model created above.\n",
    "\n",
    "f) Create a residual plot of the residuals versus the fitted values for `model_multiple` regression model. Interpret the plot.\n",
    "    \n",
    "<div>\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q9\n",
    "manual: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a)\n",
    "model_multiple = LinearRegression()\n",
    "model_multiple.fit(X = X_train[['lstat', 'rm']], y= y_train) # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b)\n",
    "model_multiple.coef_, model_multiple.intercept_ # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# medv = 1.77  - 0.673 * lstat + 4.636 * rm \n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d)\n",
    "fitted_values= model_multiple.predict(X=X_test[['lstat', 'rm']]) # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part e)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "Rsq_multiple = model_multiple.score(X = X_train[['lstat', 'rm']], y= y_train)\n",
    "\n",
    "# The R-square value of regression model created using `lstat` and `rm`\n",
    "# is better than R-square value of regression model created using `lstat` only.\n",
    "# Therefore adding more variables increases the predictive power of model.\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part f)\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "plt.scatter(fitted_values, y_test - fitted_values)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals');\n",
    "\n",
    "# This is not an example of a \"good\" residual plot. \n",
    "# There is an underlying parabolic pattern in the residuals, \n",
    "# so we may consider adding quadratic features in the model building.\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Material\n",
    "\n",
    "The [`statsmodels`](https://www.statsmodels.org/stable/index.html) is a Python package that provides R-style formula for modelling. It provide classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. To fit a regression model, we have imported the module at the top of the notebook. Below is a simple example using ordinary least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ols model with intercept\n",
    "ols_smf = smf.ols(formula='medv ~ lstat + rm', data=boston)\n",
    "\n",
    "# fit the model and summary\n",
    "ols_model = ols_smf.fit()\n",
    "# look at the summary of the model\n",
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() method provide a detailed report on the fitted model components in R-style. You can make judgment on the model performance using this report.\n",
    "\n",
    "The top of our summary starts by giving us a few details we already know. Our Dependent Variable is `medv`. Next, it details our Number of Observations in the dataset. `Df Residuals` is another name for our Degrees of Freedom in our model. This is calculated in the form of `n-k-1` or `number of observations-number of predicting variables-1`. `Df Model` is number of predicting variables. Our `Covariance Type` is listed as nonrobust. Covariance is a measure of how two variables are linked in a positive or negative manner, and a robust covariance is one that is calculated in a way to minimize or eliminate variables, which is not the case here.\n",
    "\n",
    "R-squared is possibly the most important measurement produced by this summary. R-squared is the measurement of how much of the independent variable is explained by changes in our dependent variables. In percentage terms, 0.639 would mean our model explains 63.9% of the change in our `medv` variable. Adjusted R-squared is important for analyzing multiple dependent variables efficacy on the model. \n",
    "\n",
    "The F-statistic in linear regression is comparing your produced linear model for your variables against a model that replaces your variables’ effect to 0, to find out if your group of variables are statistically significant. To interpret this number correctly, using a chosen alpha value and an F-table is necessary. Prob (F-Statistic) uses this number to tell you the accuracy of the null hypothesis, or whether it is accurate that your variables’ effect is 0. There are other components, whose discussion is out the course coverage, and [left for readers interest](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a).\n",
    "\n",
    "The method `plot_regress_exog` from statsmodels is a convenience function that gives a 2x2 plot containing the dependent variable and fitted values with confidence intervals vs. the independent variable chosen, the residuals of the model vs. the chosen independent variable, a partial regression plot, and a Component-Component plus Residual (CCPR) plot. This function can be used for quickly checking modeling assumptions with respect to a single regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(ols_model, 'lstat')\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` provide `predict` to make prediction by passing the data to as given below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=ols_model.predict(boston[['lstat','rm']])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DATA200_Worksheet_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
